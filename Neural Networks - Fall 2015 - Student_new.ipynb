{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##What is a Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks have been receiving a lot of attention lately because of their success in computer vision (e.g. Google's Deep Dream images), speech recognition and synthesis, and pattern classification. So this begs the question, \"What exactly is a neural network?\".\n",
    "\n",
    "Artificial neural networks, or ANNs, are a family of computational models inspired by the brain which are used to approximate highly complex functions. A single neural network is composed of many highly interconnected processing elements (the nodes aka neurons) which each perform some basic computation on their inputs. Each neuron typically receives many inputs from other neurons in the network, and outputs a nonlinear function of the sum of its weighted inputs. The output from this neuron is then passed through the network as input to the next set of neurons. In this way, the neural network \"deconstructs\" and parallelizes its inputs, and then \"reconstructs\" a target state or classification.\n",
    "\n",
    "There are many types of neural networks, each of which has its own connectivity and dynamics, however all neural networks share the concept of training by modifying its connection weights. It is by manipulating these weights that the neural network learns to reconstruct the desired targets in the case of supervised learning, or an efficient compression of its inputs in the case of unsupervised learning.\n",
    "\n",
    "In this tutorial, we'll take a look at one of the simplest neural networks: the multilayer perceptron (MLP), also known as the feedforward neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MultiLayerPerceptron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Shown above is a simple multilayer perceptron. In this diagram, each of the circles represents a node, or neuron, in the network. Each neuron receives input from the previous layer of neurons (except the first layer, which receives the raw input), computes a nonlinear function on the sum of its inputs, and sends its output to the next layer of neurons. Notice that there are no arrows between neurons within a layer, or going backwards between layers. For this reason, MLPs are also known as \"feedforward\" networks to distinguish them with \"recurrent\" networks, which have much more complex dynamics.\n",
    "\n",
    "The network above has three layers; the first layer is called the input layer, the second layer is known as the hidden layer, and the last layer is called the output layer. MLPs can have any number of hidden layers, but always have one input and one output layer. Each layer can be composed of any number of neurons, provided there are no connections within a layer.\n",
    "\n",
    "When the network is first initialized, random weights are assigned to the connections between the individual neurons. When the network is given an input, it uses those random connection weights to pass the activity forward one layer at a time, eventually reaching the output layer. \n",
    "\n",
    "When the activity reaches the output layer, the activity in the output layer is compared with a set of target values (thus the MLP is a supervised learning algorithm). These error values are then used to change the weights of the connections such that the error is reduced by a small amount. Over time, the error is minimized more and more until the network begins to correctly classify its input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all well and good, but how do we *actually* perform the feedforward pass? What nonlinear function should the neurons compute? In practice, the logistic function is the most often used activation function, for several reasons:\n",
    "\n",
    "* It is simple to compute\n",
    "* Its derivative is equally simple to compute (important for reasons given below)\n",
    "* It only outputs values between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **logistic function** is given by the following formula:\n",
    "\n",
    "$$f(x) = \\frac{1}{ 1 + e^{-x} }$$\n",
    "\n",
    "If we have a neuron $j$ with inputs $x_i$ and corresponding connection weights $w_{ij}$, the total input into neuron $j$ is given by:\n",
    "\n",
    "$$I_j = \\sum_{i=1}^{N} x_iw_{ij}$$\n",
    "\n",
    "Where N is the total number of inputs into neuron $j$. Hence, the output of neuron $j$ is:\n",
    "\n",
    "$$y_j = \\frac{1}{ 1 + e^{-I_j} }$$\n",
    "\n",
    "This process of summing weighted inputs and squashing them with the sigmoid function is repeated for each neuron in the network until the values of the output neurons are finally computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Backpropagation of Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to pass the activity forward through the network, we can obtain the error for each of the output neurons by comparing their activity with their target values. The question now becomes, how do we use the error information to change the weights of the network?\n",
    "\n",
    "This is done via a process called the \"backpropagation of error\". Let's first define the error measure we're trying to minimize. Here, we're going to use the sum of squared error with a factor of a half to make the derivative nice:\n",
    "\n",
    "$$E = \\frac{1}{2}\\sum_{j=1}^{N_j} (t_j - y_j)^2$$\n",
    "\n",
    "Where $y_j$ is the actual output, and $t_j$ is the target output for neuron $j$. We can take the derivative of this error to find out how the error will change as we change the activity of the output neuron.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial y_j} = -(t_j - y_j)$$\n",
    "\n",
    "This gives us an idea of *how fast* the error is changing as we change $y_i$, and in *which direction* it changes. We can use this information to adjust the input weights of a neuron such that it **decreases the error**. To do that, we need error derivatives for *all* the neurons in the network, not just the output neurons.\n",
    "\n",
    "Luckily, we can use the error derivatives with respect to the activities of the output neurons $y_j$ to compute the error derivatives with respect to the previous, hidden layer activities $y_i$. In order to do this, we must first look at how the error changes as we change the total input into neuron $j$. This is given by the chain rule.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial I_j} = \\frac{dy_j}{dI_j} \\frac{\\partial E}{\\partial y_j}$$\n",
    "\n",
    "Since $y_j$ is just the sigmoid function of $I_j$, $\\partial y_j/\\partial I_j$ is simply the derivative of the sigmoid function, which is given by:\n",
    "\n",
    "$$\\frac{\\partial y_j}{\\partial I_j} = y_j(1-y_j)$$\n",
    "\n",
    "Now we can combine this with our previously derived expression for $\\partial E/\\partial y_j$ to obtain:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial I_j} = - y_j (1-y_j) (t_j - y_j)$$\n",
    "\n",
    "Since each neuron $i$ in the hidden layer below affects **all** of the output neurons, its *total* effect on the error is given by summing its effects on each of the output neurons. This can be expressed as follows:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial y_i} = \\sum_{j=1}^{N_j} \\frac{\\partial I_j}{\\partial y_i} \\frac{\\partial E}{\\partial I_j}$$\n",
    "\n",
    "The first derivative in the sum is the change in an output neuron's total input $I_j$ as you change the activity of a neuron $y_i$ in the previous layer. But this is simply the connection weight between neuron $i$ and $j$ ! So we can express the above error derivative as:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial y_i} = \\sum_{j=1}^{N_j}w_{ij} \\frac{\\partial E}{\\partial I_j}$$\n",
    "\n",
    "We can repeat this process to get the error derivatives for every neuron in each layer of the network. So now that we have the error derivatives for all the neurons, *how do we apply the weight update?*\n",
    "\n",
    "We start by defining the change in error with respect to the connection weight $w_{ij}$. This will tell us how much a particular weight is contributing to the error, and the direction we need to change it to reduce that error. Again using our trusty chain rule:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial I_j}{\\partial w_{ij}} \\frac{\\partial E}{\\partial I_j}$$\n",
    "\n",
    "However, the first term on the right is just the activity of neuron $i$ in the layer below, and the second term we computed above. Thus we finally obtain:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{ij}} = y_i \\frac{\\partial E}{\\partial I_j}$$\n",
    "\n",
    "To change the weight $w_{ij}$, we multiply the derivative by a small number $-\\epsilon$ and add this to the current value of the weight. This will change the weight a small amount in a direction that decreases the error. The negative sign is there, because we want to *decrease* the error.\n",
    "\n",
    "$$\\Delta w_{ij} = -\\epsilon \\frac{\\partial E}{\\partial w_{ij}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Building a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea of how a neural network is able to change its weights to minimize an error function, let's build one! We're going to use Python's class system for this, which I will give a gentle introduction to. If you're already familiar with classes, the way I present things might seem strange, so bear with me.\n",
    "\n",
    "First, we want to import the numpy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define our sigmoid function and our derivative function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define our training input and output data. The number of columns in X is the number of input nodes, while the number of columns in Y is the number of output nodes. The number of hidden nodes can be chosen arbitrarily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility, let's set the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now define our synapse matrices. These are the weights. We fill them with random values between -1 and 1, remembering that they will be updated at each step during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.496410031903\n",
      "Error: 0.393897980798\n",
      "Error: 0.0286751008083\n",
      "Error: 0.00990456559616\n",
      "Error: 0.00567429108593\n",
      "Error: 0.0037718217354\n",
      "Output after training\n",
      "[[ 0.00250378]\n",
      " [ 0.99688199]\n",
      " [ 0.99713139]\n",
      " [ 0.00223687]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
